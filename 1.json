{
  "type": "container",
  "version": "0.1",
  "meta": {
    "trigger": "benchmark"
  },
  "ops": [
    {
      "id": "lmdepoy_llama_8b",
      "args": {
        "cmd": [
          "/bin/bash",
          "-c",
          "echo \"Running gpu check\"; if bench --check-gpu; then text-generation-launcher --model-id ../../root/snapshots/069adfb3ab0ceba60b9af8f11fa51558b9f9d396 --port 8080 > /dev/null 2>&1 & sleep 30; echo \"Running lm-benchmark...\"; bench --benchmark llm --url http://localhost:8080 --ping_correction --job-length 60 --model llama3.1_8B --timeout 10 --llm-framework tgi --endpoint /v1/completions --use_prompt_field --version v1.0.1; else echo \"GPU check failed. Printing nvidia-smi output:\"; nvidia-smi; fi"
        ],
        "env": {
          "CUDA_VISIBLE_DEVICES": "0",
          "CUBLAS_WORKSPACE_CONFIG": ":4096:8"
        },
        "gpu": true,
        "image": "docker.io/nosana/llm_benchmark-lmdeploy:1.0.7",
        "resources": [
          {
            "type": "S3",
            "url": "https://models.nosana.io/hugging-face/llama3.1/8b/models--unsloth--Meta-Llama-3.1-8B",
            "target": "/root/"
          }
        ]
      },
      "type": "container/run"
    }
  ]
}
